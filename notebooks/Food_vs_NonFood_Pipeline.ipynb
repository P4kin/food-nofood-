{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuWx7i2dFr0oj/OsO5QniV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"258L-lh8ASxO","executionInfo":{"status":"ok","timestamp":1761042041997,"user_tz":-420,"elapsed":33544,"user":{"displayName":"Сёма Печкин","userId":"13364738630367436528"}},"outputId":"331298b1-77fd-4f6b-9528-3e41efd6ab1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Class food: 100%|██████████| 500/500 [00:14<00:00, 34.65it/s]\n","Class non_food: 100%|██████████| 500/500 [00:18<00:00, 26.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Аугментация завершена!\n","0_aug0.jpg\n","0_aug1.jpg\n","0_aug2.jpg\n","0.jpg\n","100_aug0.jpg\n","0_aug0.jpg\n","0_aug1.jpg\n","0_aug2.jpg\n","0.jpg\n","100_aug0.jpg\n"]}],"source":["import albumentations as A\n","import cv2\n","import os\n","from tqdm import tqdm\n","\n","SRC_DIR = \"data/raw\"\n","DST_DIR = \"data/augmented\"\n","\n","# Создаём папки для аугментированных данных\n","for cls in [\"food\", \"non_food\"]:\n","    os.makedirs(os.path.join(DST_DIR, cls), exist_ok=True)\n","\n","# Трансформации\n","transform = A.Compose([\n","    A.HorizontalFlip(p=0.5),\n","    A.Rotate(limit=25, p=0.5),\n","    A.RandomBrightnessContrast(p=0.5),\n","    A.RandomResizedCrop(size=(224,224), scale=(0.8,1.0), ratio=(0.9,1.1), p=0.5)\n","])\n","\n","# Аугментация\n","for cls in [\"food\", \"non_food\"]:\n","    src_cls = os.path.join(SRC_DIR, cls)\n","    dst_cls = os.path.join(DST_DIR, cls)\n","\n","    for fname in tqdm(os.listdir(src_cls), desc=f\"Class {cls}\"):\n","        img_path = os.path.join(src_cls, fname)\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            continue\n","\n","        # Сохраняем оригинал\n","        cv2.imwrite(os.path.join(dst_cls, fname), img)\n","\n","        # Генерируем 3 новых аугментированных версии\n","        for i in range(3):\n","            augmented = transform(image=img)['image']\n","            name, ext = os.path.splitext(fname)\n","            cv2.imwrite(os.path.join(dst_cls, f\"{name}_aug{i}{ext}\"), augmented)\n","\n","print(\"Аугментация завершена!\")\n","!ls data/augmented/food | head -n 5\n","!ls data/augmented/non_food | head -n 5"]},{"cell_type":"code","source":["# Установка библиотек\n","!pip install torch torchvision torchaudio --quiet\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","import os\n","import csv\n","\n","# -------------------------------\n","# 1. Параметры обучения\n","# -------------------------------\n","BATCH_SIZE = 32\n","EPOCHS = 3\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# -------------------------------\n","# 2. Трансформации для данных\n","# -------------------------------\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","])\n","\n","# -------------------------------\n","# 3. Загружаем датасет\n","# Можно менять между data/raw и data/augmented\n","# -------------------------------\n","DATA_DIR = \"data/augmented\"\n","train_dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","# -------------------------------\n","# 4. Определяем модели\n","# -------------------------------\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.MaxPool2d(2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128*28*28, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 2)\n","        )\n","    def forward(self,x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","# ResNet18 и EfficientNet_B0 с pretrained=True\n","resnet18 = models.resnet18(pretrained=True)\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n","\n","efficientnet = models.efficientnet_b0(pretrained=True)\n","efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, 2)\n","\n","models_dict = {\n","    \"SimpleCNN\": SimpleCNN(),\n","    \"ResNet18\": resnet18,\n","    \"EfficientNet_B0\": efficientnet\n","}\n","\n","# -------------------------------\n","# 5. Функция тренировки с локальным логом\n","# -------------------------------\n","def train_model(model, loader, epochs=EPOCHS, log_file=\"training_log.csv\"):\n","    model = model.to(DEVICE)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","    # Создаём CSV для логирования\n","    with open(log_file, \"a\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"Model\", \"Epoch\", \"Loss\", \"Accuracy\"])\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for imgs, labels in loader:\n","            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","            optimizer.zero_grad()\n","            outputs = model(imgs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * imgs.size(0)\n","            _, predicted = torch.max(outputs,1)\n","            correct += (predicted==labels).sum().item()\n","            total += labels.size(0)\n","\n","        epoch_loss = running_loss / total\n","        epoch_acc = correct / total\n","        print(f\"{model.__class__.__name__} Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n","\n","        # Сохраняем в CSV\n","        with open(log_file, \"a\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            writer.writerow([model.__class__.__name__, epoch+1, epoch_loss, epoch_acc])\n","\n","    return model\n","\n","# -------------------------------\n","# 6. Обучение всех моделей\n","# -------------------------------\n","for name, model in models_dict.items():\n","    print(f\"=== Training {name} ===\")\n","    train_model(model, train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnyqGF1vSuba","executionInfo":{"status":"ok","timestamp":1761050207665,"user_tz":-420,"elapsed":6088435,"user":{"displayName":"Сёма Печкин","userId":"13364738630367436528"}},"outputId":"8d566d27-a5de-48c3-c53d-bf375365b573"},"execution_count":22,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 44.7M/44.7M [00:01<00:00, 32.3MB/s]\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 20.5M/20.5M [00:00<00:00, 22.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["=== Training SimpleCNN ===\n","SimpleCNN Epoch 1/3 - Loss: 0.4925, Acc: 0.7540\n","SimpleCNN Epoch 2/3 - Loss: 0.3618, Acc: 0.8430\n","SimpleCNN Epoch 3/3 - Loss: 0.2890, Acc: 0.8790\n","=== Training ResNet18 ===\n","ResNet Epoch 1/3 - Loss: 0.0648, Acc: 0.9762\n","ResNet Epoch 2/3 - Loss: 0.0059, Acc: 0.9982\n","ResNet Epoch 3/3 - Loss: 0.0044, Acc: 0.9990\n","=== Training EfficientNet_B0 ===\n","EfficientNet Epoch 1/3 - Loss: 0.1792, Acc: 0.9435\n","EfficientNet Epoch 2/3 - Loss: 0.0233, Acc: 0.9965\n","EfficientNet Epoch 3/3 - Loss: 0.0085, Acc: 0.9990\n"]}]},{"cell_type":"code","source":["!pip install torchvision --quiet\n","\n","import os, csv, torch\n","import torch.nn as nn, torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score\n","\n","# ==== Параметры ====\n","DATA_DIR = \"data/raw\"\n","EPOCHS = 3\n","BATCH_SIZE = 32\n","LR = 1e-4\n","LOG_FILE = \"training_log_raw.csv\"\n","MODEL_DIR = \"models\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","# ==== Препроцессинг (без аугментаций) ====\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n","val_split = int(0.2 * len(dataset))\n","train_split = len(dataset) - val_split\n","\n","if val_split == 0:\n","    train_dataset = dataset\n","    val_dataset = dataset\n","else:\n","    from torch.utils.data import random_split\n","    train_dataset, val_dataset = random_split(dataset, [train_split, val_split])\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n","print(f\"Device: {DEVICE}\")\n","\n","# ==== SimpleCNN ====\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.MaxPool2d(2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128*28*28, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, len(dataset.classes))\n","        )\n","    def forward(self,x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","# ==== Модели ====\n","def make_models(num_classes):\n","    simple = SimpleCNN()\n","    res = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","    res.fc = nn.Linear(res.fc.in_features, num_classes)\n","    eff = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n","    eff.classifier[1] = nn.Linear(eff.classifier[1].in_features, num_classes)\n","    return {\"SimpleCNN\": simple, \"ResNet18\": res, \"EfficientNet_B0\": eff}\n","\n","# ==== Оценка ====\n","def evaluate_model(model, loader, device):\n","    model.eval()\n","    ys, preds = [], []\n","    with torch.no_grad():\n","        for xb, yb in loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            out = model(xb)\n","            pred = torch.argmax(out, dim=1)\n","            ys.extend(yb.cpu().numpy().tolist())\n","            preds.extend(pred.cpu().numpy().tolist())\n","    return accuracy_score(ys, preds)\n","\n","# ==== Обучение + лог ====\n","def train_and_log(name, model, train_loader, val_loader, epochs, log_file, dataset_tag):\n","    model = model.to(DEVICE)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","\n","    write_header = not os.path.exists(log_file)\n","    if write_header:\n","        with open(log_file, \"w\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            writer.writerow([\"Model\", \"Dataset\", \"Epoch\", \"Loss\", \"Accuracy\"])\n","\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        running_loss, correct, total = 0.0, 0, 0\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n","            optimizer.zero_grad()\n","            out = model(xb)\n","            loss = criterion(out, yb)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item() * xb.size(0)\n","            _, predicted = torch.max(out,1)\n","            correct += (predicted==yb).sum().item()\n","            total += yb.size(0)\n","        epoch_loss = running_loss / total if total>0 else 0.0\n","        epoch_acc = correct / total if total>0 else 0.0\n","        val_acc = evaluate_model(model, val_loader, DEVICE) if len(val_loader)>0 else epoch_acc\n","\n","        print(f\"{name} Epoch {epoch}/{epochs} - Loss: {epoch_loss:.4f}, TrainAcc: {epoch_acc:.4f}, ValAcc: {val_acc:.4f}\")\n","\n","        with open(log_file, \"a\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            writer.writerow([name, dataset_tag, epoch, epoch_loss, val_acc])\n","\n","    # сохраняем модель отдельно\n","    out_path = os.path.join(MODEL_DIR, f\"{name}_raw.pth\")\n","    torch.save(model.state_dict(), out_path)\n","    print(f\" Model saved: {out_path}\")\n","    return model\n","\n","# ==== Запуск эксперимента ====\n","num_classes = len(dataset.classes)\n","models_dict_new = make_models(num_classes)\n","\n","for name, model in models_dict_new.items():\n","    print(f\"=== Training {name} on RAW data ===\")\n","    trained = train_and_log(name, model, train_loader, val_loader, EPOCHS, LOG_FILE, dataset_tag=\"raw\")\n","    del trained\n","    torch.cuda.empty_cache()\n","\n","print(f\"\\nЭксперимент на data/raw завершён \\nЛоги сохранены в {LOG_FILE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CA0DGLxxI-Zk","executionInfo":{"status":"ok","timestamp":1761058147609,"user_tz":-420,"elapsed":1736411,"user":{"displayName":"Сёма Печкин","userId":"13364738630367436528"}},"outputId":"3057acad-ecc4-4de0-b7f2-bbfea323a402"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Train samples: 800, Val samples: 200\n","Device: cpu\n","=== Training SimpleCNN on RAW data ===\n","SimpleCNN Epoch 1/3 - Loss: 0.5597, TrainAcc: 0.7113, ValAcc: 0.7050\n","SimpleCNN Epoch 2/3 - Loss: 0.4175, TrainAcc: 0.8187, ValAcc: 0.7750\n","SimpleCNN Epoch 3/3 - Loss: 0.3381, TrainAcc: 0.8575, ValAcc: 0.8150\n","✅ Model saved: models/SimpleCNN_raw.pth\n","=== Training ResNet18 on RAW data ===\n","ResNet18 Epoch 1/3 - Loss: 0.1682, TrainAcc: 0.9337, ValAcc: 0.9400\n","ResNet18 Epoch 2/3 - Loss: 0.0261, TrainAcc: 0.9912, ValAcc: 0.9600\n","ResNet18 Epoch 3/3 - Loss: 0.0056, TrainAcc: 1.0000, ValAcc: 0.9750\n","✅ Model saved: models/ResNet18_raw.pth\n","=== Training EfficientNet_B0 on RAW data ===\n","EfficientNet_B0 Epoch 1/3 - Loss: 0.4420, TrainAcc: 0.8462, ValAcc: 0.9450\n","EfficientNet_B0 Epoch 2/3 - Loss: 0.1509, TrainAcc: 0.9738, ValAcc: 0.9750\n","EfficientNet_B0 Epoch 3/3 - Loss: 0.0727, TrainAcc: 0.9900, ValAcc: 0.9750\n","✅ Model saved: models/EfficientNet_B0_raw.pth\n","\n","Эксперимент на data/raw завершён ✅\n","Логи сохранены в training_log_raw.csv\n"]}]},{"cell_type":"code","source":["\n","!git add /content/food-nofood-/notebooks/Food_vs_NonFood_Pipeline.ipynb\n","!git commit -m \"Обновлён ноутбук с последними экспериментами\"\n","!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9CstDLyl8yf","executionInfo":{"status":"ok","timestamp":1761065018398,"user_tz":-420,"elapsed":824,"user":{"displayName":"Сёма Печкин","userId":"13364738630367436528"}},"outputId":"957200ee-d36f-4cb3-e2a0-f1fdea4d0f5e"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mdata/\u001b[m\n","\t\u001b[31mfood5k-image-dataset.zip\u001b[m\n","\t\u001b[31msrc/\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n","Everything up-to-date\n"]}]}]}